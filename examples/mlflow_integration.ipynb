{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow Integration Example\n",
    "\n",
    "This notebook demonstrates how to use ml-assert's MLflow integration to track assertion results and model metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ml_assert.core.base import AssertionResult\n",
    "from ml_assert.core.dsl import ModelAssertion\n",
    "from ml_assert.integrations.mlflow import MLflowLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Setup\n",
    "\n",
    "First, let's create some sample data and train a simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "np.random.seed(42)\n",
    "X = pd.DataFrame(\n",
    "    {\n",
    "        \"feature1\": np.random.normal(0, 1, 1000),\n",
    "        \"feature2\": np.random.normal(0, 1, 1000),\n",
    "        \"feature3\": np.random.normal(0, 1, 1000),\n",
    "    }\n",
    ")\n",
    "y = (X[\"feature1\"] + X[\"feature2\"] > 0).astype(int)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_scores = model.predict_proba(X_test)[:, 1]  # Get probability scores for ROC AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLflow Integration\n",
    "\n",
    "Now let's set up the MLflow logger and run some assertions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MLflow logger\n",
    "mlflow_logger = MLflowLogger(\n",
    "    experiment_name=\"ml-assert-example\", run_name=\"model-validation-run\"\n",
    ")\n",
    "\n",
    "# Start a new run\n",
    "mlflow_logger.start_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running Assertions and Logging Results\n",
    "\n",
    "Let's run some model performance assertions and log the results to MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model performance assertions\n",
    "model_assertion = ModelAssertion(y_test, y_pred)\n",
    "model_assertion._y_scores = y_scores  # Set scores for ROC AUC\n",
    "result = (\n",
    "    model_assertion.accuracy(threshold=0.8)\n",
    "    .precision(threshold=0.75)\n",
    "    .recall(threshold=0.75)\n",
    "    .f1(threshold=0.75)\n",
    "    .roc_auc(threshold=0.8)\n",
    "    .validate()\n",
    ")\n",
    "\n",
    "# Log the assertion result\n",
    "mlflow_logger.log_assertion_result_mlassert(\n",
    "    result=result, step_name=\"model_performance\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logging Multiple Assertions\n",
    "\n",
    "Let's create and log multiple assertion results for different aspects of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create assertion results for different metrics\n",
    "accuracy_result = AssertionResult(\n",
    "    success=True,\n",
    "    message=\"Accuracy check passed\",\n",
    "    timestamp=datetime.now(),\n",
    "    metadata={\"accuracy\": 0.85},\n",
    ")\n",
    "\n",
    "fairness_result = AssertionResult(\n",
    "    success=False,\n",
    "    message=\"Fairness check failed\",\n",
    "    timestamp=datetime.now(),\n",
    "    metadata={\"demographic_parity\": 0.15},\n",
    ")\n",
    "\n",
    "# Log multiple assertion results\n",
    "mlflow_logger.log_assertion_result_mlassert(accuracy_result, step_name=\"accuracy_check\")\n",
    "mlflow_logger.log_assertion_result_mlassert(fairness_result, step_name=\"fairness_check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using Context Manager\n",
    "\n",
    "The MLflowLogger can also be used as a context manager for cleaner code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using context manager\n",
    "with mlflow_logger.run() as logger:\n",
    "    # Run some assertions\n",
    "    result = AssertionResult(\n",
    "        success=True,\n",
    "        message=\"Feature importance check passed\",\n",
    "        timestamp=datetime.now(),\n",
    "        metadata={\n",
    "            \"feature1_importance\": 0.4,\n",
    "            \"feature2_importance\": 0.35,\n",
    "            \"feature3_importance\": 0.25,\n",
    "        },\n",
    "    )\n",
    "    logger.log_assertion_result_mlassert(result, step_name=\"feature_importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. End the Run\n",
    "\n",
    "Finally, let's end the MLflow run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_logger.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Viewing Results in MLflow UI\n",
    "\n",
    "You can now view the logged results in the MLflow UI by running:\n",
    "```bash\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "The UI will show:\n",
    "- All assertion results with their success/failure status\n",
    "- Metrics and parameters logged for each assertion\n",
    "- Timestamps and messages for each assertion\n",
    "- Metadata associated with each assertion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
